# -*- coding: utf-8 -*-
"""Hometask 01. Gradient Descent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x8y1Fs9riMEyxwKNjnFXAZvzd2WRilub

# Домашнее задание.

На всякий случай напомним, что использование открытых источников (medium, towardsdatascience и т. д.) разрешено только при указании ссылки первоисточника. Вполне вероятно, что более одного человека нагуглят одинаковый код. Указание ссылки позволит нам не помечать такие работы как списывание.

Не забывайте и правила использования моделей по типу Copilot/ChatGPT и их аналогов: не более 30% кода и полное указание промпта для модели. Если просили модель что-то корректировать, или же сами что-то правили -- всё нужно указывать.

## Часть 1. Библиотека NumPy

### Формат сдачи

Задание выполняем в Google Colab и присылаем ссылку с включенным режимом комментирования.

### О задании

В данном домашнем задании вы попрактикуетесь в работе с библиотекой numpy!
"""

import numpy as np

"""Во всех задачах необходимо написать код решения внутри функции и проверить его работу, вызвав функцию для данных из условия.

При решении задач запрещается использовать циклы (`for`, `while`) и оператор `if`.

Везде, где встречаются массивы или матрицы, подразумевается, что это `numpy.array`.

### Задание 1
Напишите функцию, возвращающую округленную взвешенную сумму оценок по данным оценкам и весам.
Можете посчитать свою оценку за курс :)
"""
# %%
def result_mark(weights, marks):
    return sum(weights[::] * marks[::])

weights = np.array([0.35, 0.4, 0.15, 0.1])
marks = np.array([6, 9, 8, 7])
result_mark(weights, marks)

"""
### Задание 2
Напишите функцию, меняющую каждое третье (начиная с 0) значение массива целых чисел на заданное число.
Например, если на вход поступает массив `array([3, 5, 1, 0, -3, 22, 213436])` и число `-111`,
то на выходе должен получиться массив `array([-111, 5, 1, -111, -3, 22, -111])`.
"""
# %%
def change_array(array, number):
    array[::3] = number

array = np.array([3, 5, 1, 0, -3, 22, 213436])
number = -111
change_array(array, number)
print(array)

"""### Задание 3

Напишите функцию, выдающую индексы «близких» элементов заданных массивов, а именно тех пар элементов, чей модуль разницы не превосходит заданного значения. Например, если на вход поступают массив `array([1.5, 0.5, 2, -4.1, -3, 6, -1])`, массив `array([1.2, 0.5, 1, -4, 3, 0, -1.2])` и число `0.5`, то на выходе должен получиться массив `array([0, 1, 3, 6])` (_**важно: не `tuple`, а одномерный массив типа `numpy.ndarray` (то есть `.ndim` от него равно 1)!**_).
"""

def find_close(array1, array2, precision):
    # your code here

array1 = np.array([1.5, 0.5, 2, -4.1, -3, 6, -1])
array2 = np.array([1.2, 0.5, 1, -4, 3, 0, -1.2])
precision = 0.5
find_close(array1, array2, precision)

"""### Задание 4

Напишите функцию, которая составляет блочную матрицу из четырех блоков, где каждый блок - это заданная матрица. Например, если на вход поступает матрица
$$
\begin{pmatrix}
0 & 1 & 2\\
3 & 4 & 5\\
\end{pmatrix},
$$
то ответом будет матрица
$$
\begin{pmatrix}
0 & 1 & 2 & 0 & 1 & 2\\
3 & 4 & 5 & 3 & 4 & 5\\
0 & 1 & 2 & 0 & 1 & 2\\
3 & 4 & 5 & 3 & 4 & 5\\
\end{pmatrix}
$$
"""

def block_matrix(block):
    # your code here

block = np.array([[0, 1, 2], [3, 4, 5]])
block_matrix(block)

"""### Задание 5

Напишите функцию, вычисляющую произведение всех ненулевых диагональных элементов на диагонали данной квадратной матрицы. Например, если на вход поступает матрица
$$
\begin{pmatrix}
0 & 1 & 2\\
3 & 4 & 5\\
6 & 7 & 8\\
\end{pmatrix},
$$
то ответом будет 32.
"""

def diag_prod(matrix):
    # your code here

matrix = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
diag_prod(matrix)

"""### Задание 6

Для улучшения качества работы некоторых алгоритмов машинного обучения может быть полезно использовать [нормализацию данных](https://vk.cc/8xmfQk), чтобы привести признаки в выборке к одному масштабу — а именно, из каждого столбца вычесть среднее его значений и поделить на их стандартное отклонение. Напишите функцию, нормализующую входящую матрицу (по столбцам). Например, если на вход подается матрица
$$
\begin{pmatrix}
1 & 4 & 4200\\
0 & 10 & 5000\\
1 & 2 & 1000\\
\end{pmatrix},
$$
то результатом с точностью до сотых будет матрица
$$
\begin{pmatrix}
0.71 & -0.39 & 0.46\\
-1.41 & 1.37 & 0.93\\
0.71 & -0.98 & -1.39\\
\end{pmatrix}
$$

_Подсказка. Учтите, что в вашей матрице не должно получаться никаких nan. Подумайте, в каком случае они могут возникнуть и как обойти эту проблему._
"""

def normalize(matrix):
    # your code here

matrix = np.array([[1, 4, 4200], [0, 10, 5000], [1, 2, 1000]])
normalize(matrix)

"""### Задание 7

Напишите функцию, вычисляющую какую-нибудь первообразную данного полинома (в качестве константы возьмите ваше любимое число). Например, если на вход поступает массив коэффициентов `array([4, 6, 0, 1])`, что соответствует полиному $4x^3 + 6x^2 + 1$, на выходе получается массив коэффициентов `array([1, 2, 0, 1, -2])`, соответствующий полиному $x^4 + 2x^3 + x - 2$.
"""

def antiderivative(coefs):
    # your code here

coefs = np.array([4, 6, 0, 1])
antiderivative(coefs)

"""### Задание 8

Напишите функцию, делающую данную [треугольную матрицу](https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B5%D1%83%D0%B3%D0%BE%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0) симметричной. Например, если на вход поступает матрица
$$
\begin{pmatrix}
1 & 2 & 3 & 4\\
0 & 5 & 6 & 7\\
0 & 0 & 8 & 9\\
0 & 0 & 0 & 10\\
\end{pmatrix},
$$
то на выходе должна быть матрица
$$
\begin{pmatrix}
1 & 2 & 3 & 4\\
2 & 5 & 6 & 7\\
3 & 6 & 8 & 9\\
4 & 7 & 9 & 10\\
\end{pmatrix}.
$$
"""

def make_symmetric(matrix):
    # your code here

matrix = np.array([[1, 2, 3, 4], [0, 5, 6, 7], [0, 0, 8, 9], [0, 0, 0, 10]])
make_symmetric(matrix)

"""### Задание 9

Напишите функцию, создающую прямоугольную матрицу из m одинаковых строк, заполненных последовательными натуральными числами от a до b включительно в возрастающем порядке. Например, если m = 5, a = 3, b = 10, то на выходе будет матрица
$$
\begin{pmatrix}
3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
\end{pmatrix}
$$
"""

def construct_matrix(m, a, b):
    # your code here

m = 5
a = 3
b = 10
construct_matrix(m, a, b)

"""### Задание 10

Напишите функцию, вычисляющую [косинусную близость](https://en.wikipedia.org/wiki/Cosine_similarity) двух векторов. Например, если на вход поступают вектора `array([-2, 1, 0, -5, 4, 3, -3])` и `array([0, 2, -2, 10, 6, 0, 0])`, ответом будет -0.25.
"""

def cosine_similarity(vec1, vec2):
    # your code here

vec1 = np.array([-2, 1, 0, -5, 4, 3, -3])
vec2 = np.array([0, 2, -2, 10, 6, 0, 0])
cosine_similarity(vec1, vec2)

"""## Часть 2. Градиентный спуск

Имеем 1000 объектов и 10 признаков у каждого (+таргет)!

Обучим модель линейной регрессии:

$$
a(x) = \beta_1 d_{1} + \beta_2 d_{2} + \beta_3 d_{3} + \beta_4 d_{4} + \beta_5 d_{5} + \beta_6 d_{6} + \beta_7 d_{7} + \beta_8 d_{8} + \beta_9 d_{9} + \beta_{10} d_{10} + \beta_0
$$

Которая минимизирует MSE:

$$
Q(a(X), Y) = \sum_i^{1000} (a(x_i) - y_i)^2
$$
"""

import pandas as pd
data = pd.read_csv('https://www.dropbox.com/scl/fi/kn9ow3wg470op7svd4j4f/gd_practice_data.csv?rlkey=bods94fgcndn6vuydsm0bao7v&st=ei4otwq6&dl=1')

data.head()

data.describe().T # выводим описательные статистики

"""Обучим коэффициенты линейной регрессии с помощью библиотеки <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"> **sklearn** </a>

Отдельно выведем оценку свободного коэффициента  ($\beta_0$ при $d_0 = 1$)
"""

from sklearn.linear_model import LinearRegression
X = data.drop('target', axis=1)
Y = data.target

model = LinearRegression()
model.fit(X, Y)

"""Посмотрим на веса модели:"""

model.coef_

model.intercept_

"""Теперь вам необходимо реализовать функции для оптимизации коэффициентов линейной регрессии МНК.
Подразумевается, что на вход алгоритм будет принимать следующие параметры:

- 2 pandas датафрейма **samples** и **targets**, содержащих матрицу объектов и ветор ответов соответственно
- значение **learning rate**, который корректирует длину вектора-градиента (чтобы он не взорвался)
- значение **threshold**'а для критерия останова (когда мы считаем, что мы сошлись к оптимуму)
- значение **max_iter**, ограничивающее количество итераций градиентного спуска

Ниже есть следующие заготовки функций, которые надо дополнить:

- **add_constant_feature**: добавляет колонку с названием *constant* из единичек к переданному датафрейму **samples**. Это позволяет оценить свободный коэффициент $\beta_0$.

- **calculate_mse_loss**: вычисляет при текущих весах **self.beta** значение среднеквадратической ошибки.

- **calculate_gradient**: вычисляет при текущих весах вектор-градиент по функционалу.

- **iteration**: производит итерацию градиентного спуска, то есть обновляет веса модели, в соответствии с установленным **learning_rate = $\eta$**: $\beta^{(n+1)} = \beta^{(n)} - \eta \cdot \nabla Q(\beta^{(n)})$

- **learn**: производит итерации обучения до того момента, пока не сработает критерий останова обучения. В этот раз критерием останова будет следующее событие: во время крайней итерации изменение в функционале качества модели составило значение меньшее, чем **self.threshold**. Иными словами, $|Q(\beta^{(n)}) - Q(\beta^{(n+1)})| < threshold$.

### Hint: пример вычисления производной

$$
Q(a, X) = \frac{1}{N}\cdot\sum_{i=1}^N (\beta_1 \cdot d_{i1} + ... + \beta_n \cdot d_{in} - y_i)^2
$$

Выше - минимизируемая функция. Она зависит от n переменных: $\beta_1, ..., \beta_n$. Вектор-градиент - матрица с одной строчкой, состоящей из производных 1го порядка по всем переменным.

$$
\nabla Q(a, X) = (Q'_{\beta_1} \;\;\; Q'_{\beta_2} \;\;\; ... \;\;\; Q'_{\beta_{n-1}}  \;\;\;  Q'_{\beta_n})
$$

Пример вычисления производной по первой переменной:

$$
Q'_{\beta_1} = \frac{2}{N} \cdot \sum_{i=1}^N d_{i1} (\beta_1 \cdot d_{i1} + ... + \beta_{n} \cdot d_{in} - y_i)
$$

Скажем, для нашего датасета X, Y вычислим эту саму производную при начальных единичных коэффициентах $\beta_{start} = (1 \;\;\; 1 \;\;\; ...)$



Получим для каждого объекта в начале выражение из скобочек:
$$
\beta_1 \cdot d_{i1} + ... + \beta_{n} \cdot d_{in} - y_i
$$
"""

### Инициализируем точку для коэффициентов в виде вектора из единичек
initial_betas = np.ones(X.shape[1])

### Получим выражение выше для каждого объекта.
### Для этого скалярно перемножим строчки из X на наши beta

scalar_value = np.dot(X, initial_betas.reshape(-1, 1)).ravel()
scalar_value = (scalar_value - Y).values

"""Теперь полученное значение для каждого объекта умножим на соответствующее значение признака $d_1$:

$$
d_{i1} \cdot (\beta_1 \cdot d_{i1} + ... + \beta_{n} \cdot d_{in} - y_i)
$$
"""

### Возьмем столбик со значениями 1 признака

d_i1 = X.values[:, 0]

### Умножим каждый объект на соответствующее значение признака
scalar_value = scalar_value * d_i1

### Наконец, умножим все на 2 и усреднимся,
### чтобы получить значение производной по первому параметру

2 * np.mean(scalar_value)

"""### Эта логика поможем Вам при реализации!

Также старайтесь избегать циклов там, где они не нужны. Не забыли же, что циклы медленные в питоне? :)
"""

# вычисляет при текущих весах self.beta значение среднеквадратической ошибки.
def calculate_mse_loss(beta: np.ndarray, samples: np.ndarray, targets: np.ndarray) -> float:
    """
    Метод для расчета среднеквадратической ошибки.

    :return: среднеквадратическая ошибка при текущих весах модели : float
    """
    # Вычисление MSE на основе текущих весов и таргетов
    loss = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
    return loss

# добавляет колонку с названием constant из единичек к переданному датафрейму samples. Это позволяет оценить свободный коэффициент  β0
def add_constant_feature(samples: np.ndarray, beta: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    Функция для создания константной фичи в матрице объектов samples.
    Добавляет колонку с единицами и обновляет вектор весов.
    """
    # А здесь мы уже всё написали для Вас :)

    # Добавление константной колонки к матрице признаков
    samples['constants'] = 1

    # Обновление вектора весов
    beta = np.append(beta, 1)

    return samples, beta

# вычисляет при текущих весах вектор-градиент по функционалу.
def calculate_gradient(beta: np.ndarray, samples: np.ndarray, targets: np.ndarray) -> np.ndarray:
    """
    Функция для вычисления вектора-градиента.

    :return: вектор-градиент, содержащий производные по каждому признаку : np.ndarray
    """
    # Вычисление разницы между предсказанными и фактическими значениями
    diff = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

    # Вычисление градиента (не забудьте 2 и усреднить по количеству образцов!)
    # Решение на полный балл предполагает запись в матричной форме, без циклов
    gradient =  # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

    return gradient

#  производит итерацию градиентного спуска, то есть обновляет веса модели, в соответствии с установленным learning_rate
def iteration(beta: np.ndarray, samples: np.ndarray, targets: np.ndarray, learning_rate: float) -> np.ndarray:
    """
    Обновляем веса модели в соответствии с текущим вектором-градиентом.
    """
    # Обновление весов на основе вычисленного градиента
    gradient = calculate_gradient(beta, samples, targets)

    # Обновите веса модели используя градиент выше. Не перепутайте знак :)
    new_beta = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

    return new_beta

# производит итерации обучения до того момента, пока не сработает критерий останова обучения.
def learn(samples: pd.DataFrame, targets: pd.DataFrame,
                learning_rate: float = 1e-3, threshold: float=1e-6, max_iter: int = 100000) -> tuple[np.ndarray, int, dict]:
    """
    Итеративное обучение весов модели до срабатывания критерия останова.

    Запись mse и номера итерации в iteration_loss_dict.
    """

    # Сохранение матрицы признаков и таргетов
    samples = samples.copy()
    targets = targets.copy()

    # Инициализация весов
    beta = np.ones(samples.shape[1])

    # Добавим константу и перейдем к numpy
    samples, beta = add_constant_feature(samples, beta)
    samples = np.array(samples)

    # Преобразование таргетов в одномерный массив
    targets = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

    # Инициализация словаря для хранения MSE
    iteration_loss_dict = {0 : calculate_mse_loss(beta, samples, targets)}

    for itera in range(1, max_iter+1):
        start_betas = beta.copy()  # Фиксируем текущие веса

        # Выполняем шаг градиентного спуска при помощи iteration
        new_beta = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

        # Рассчитываем новую MSE
        new_mse = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

        # Записываем MSE и номер итерации
        iteration_loss_dict[itera] = new_mse

        # Проверяем условие остановки по изменению MSE
        if # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ
            break

        # Обновляем предыдущее значение весов
        beta = # Ваш код  ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ

        # Выводим MSE в реальном времени
        print("MSE:", new_mse, "                   ", end="\r")

    if itera == max_iter:
        print("Кажется алгоритм расходится. Возможно, у Вас где-то ошибка :(")

    return beta, itera, iteration_loss_dict

"""Обучим коэффициенты линейной модели с помощью реализованного нами градиентного спуска, не забыв добавить свободную переменную. Получились ли такие же коэффициенты, как и при использовании **LinearRegression** из **sklearn**? Если нет, то почему они отличаются, на Ваш взгляд, и сильно ли?

Последнее число соответствует свободному коэффициенту, он же **.intercept_** в **LinearRegression** из **sklearn**
"""

# Запустим обучение
beta, iteration_num, iteration_loss_dict = learn(X, Y)

print('Веса модели при переменных d1, d2, ..., d10 равны соответственно: \n\n' + str(beta))